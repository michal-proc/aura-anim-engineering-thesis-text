
@misc{reda_film_2022,
	title = {{FILM}: {Frame} {Interpolation} for {Large} {Motion}},
	shorttitle = {{FILM}},
	url = {http://arxiv.org/abs/2202.04901},
	doi = {10.48550/arXiv.2202.04901},
	abstract = {We present a frame interpolation algorithm that synthesizes multiple intermediate frames from two input images with large in-between motion. Recent methods use multiple networks to estimate optical flow or depth and a separate network dedicated to frame synthesis. This is often complex and requires scarce optical flow or depth ground-truth. In this work, we present a single unified network, distinguished by a multi-scale feature extractor that shares weights at all scales, and is trainable from frames alone. To synthesize crisp and pleasing frames, we propose to optimize our network with the Gram matrix loss that measures the correlation difference between feature maps. Our approach outperforms state-of-the-art methods on the Xiph large motion benchmark. We also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing to methods that use perceptual losses. We study the effect of weight sharing and of training with datasets of increasing motion range. Finally, we demonstrate our model's effectiveness in synthesizing high quality and temporally coherent videos on a challenging near-duplicate photos dataset. Codes and pre-trained models are available at https://film-net.github.io.},
	urldate = {2025-06-05},
	publisher = {arXiv},
	author = {Reda, Fitsum and Kontkanen, Janne and Tabellion, Eric and Sun, Deqing and Pantofaru, Caroline and Curless, Brian},
	month = jul,
	year = {2022},
	note = {arXiv:2202.04901 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\L2FA7JL3\\Reda et al. - 2022 - FILM Frame Interpolation for Large Motion.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\QA7IXCA2\\2202.html:text/html},
}

@misc{wang_esrgan_2018,
	title = {{ESRGAN}: {Enhanced} {Super}-{Resolution} {Generative} {Adversarial} {Networks}},
	shorttitle = {{ESRGAN}},
	url = {http://arxiv.org/abs/1809.00219},
	doi = {10.48550/arXiv.1809.00219},
	abstract = {The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at https://github.com/xinntao/ESRGAN .},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
	month = sep,
	year = {2018},
	note = {arXiv:1809.00219 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\XN35CQAG\\Wang et al. - 2018 - ESRGAN Enhanced Super-Resolution Generative Adversarial Networks.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\AU38MYI5\\1809.html:text/html},
}

@misc{wang_real-esrgan_2021,
	title = {Real-{ESRGAN}: {Training} {Real}-{World} {Blind} {Super}-{Resolution} with {Pure} {Synthetic} {Data}},
	shorttitle = {Real-{ESRGAN}},
	url = {http://arxiv.org/abs/2107.10833},
	doi = {10.48550/arXiv.2107.10833},
	abstract = {Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
	month = aug,
	year = {2021},
	note = {arXiv:2107.10833 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\HI28NV8M\\Wang et al. - 2021 - Real-ESRGAN Training Real-World Blind Super-Resolution with Pure Synthetic Data.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\N59RYAT3\\2107.html:text/html},
}

@misc{wang_error_2025,
	title = {Error {Analyses} of {Auto}-{Regressive} {Video} {Diffusion} {Models}: {A} {Unified} {Framework}},
	shorttitle = {Error {Analyses} of {Auto}-{Regressive} {Video} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2503.10704},
	doi = {10.48550/arXiv.2503.10704},
	abstract = {A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.},
	urldate = {2025-06-03},
	publisher = {arXiv},
	author = {Wang, Jing and Zhang, Fengzhuo and Li, Xiaoli and Tan, Vincent Y. F. and Pang, Tianyu and Du, Chao and Sun, Aixin and Yang, Zhuoran},
	month = mar,
	year = {2025},
	note = {arXiv:2503.10704 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\HR29SJ75\\Wang et al. - 2025 - Error Analyses of Auto-Regressive Video Diffusion Models A Unified Framework.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\T2BAGL6Z\\2503.html:text/html},
}

@misc{huang_free-bloom_2023,
	title = {Free-{Bloom}: {Zero}-{Shot} {Text}-to-{Video} {Generator} with {LLM} {Director} and {LDM} {Animator}},
	shorttitle = {Free-{Bloom}},
	url = {http://arxiv.org/abs/2309.14494},
	doi = {10.48550/arXiv.2309.14494},
	abstract = {Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of "moving images", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Huang, Hanzhuo and Feng, Yufan and Shi, Cheng and Xu, Lan and Yu, Jingyi and Yang, Sibei},
	month = sep,
	year = {2023},
	note = {arXiv:2309.14494 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\92W9UTZH\\Huang et al. - 2023 - Free-Bloom Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\3I6BBNTD\\2309.html:text/html},
}

@misc{lu_flowzero_2023,
	title = {{FlowZero}: {Zero}-{Shot} {Text}-to-{Video} {Synthesis} with {LLM}-{Driven} {Dynamic} {Scene} {Syntax}},
	shorttitle = {{FlowZero}},
	url = {http://arxiv.org/abs/2311.15813},
	doi = {10.48550/arXiv.2311.15813},
	abstract = {Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Lu, Yu and Zhu, Linchao and Fan, Hehe and Yang, Yi},
	month = nov,
	year = {2023},
	note = {arXiv:2311.15813 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\B4YJHMTE\\Lu et al. - 2023 - FlowZero Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\BFHX27KN\\2311.html:text/html},
}

@misc{qin_dancing_2023,
	title = {Dancing {Avatar}: {Pose} and {Text}-{Guided} {Human} {Motion} {Videos} {Synthesis} with {Image} {Diffusion} {Model}},
	shorttitle = {Dancing {Avatar}},
	url = {http://arxiv.org/abs/2308.07749},
	doi = {10.48550/arXiv.2308.07749},
	abstract = {The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT. For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques. Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame. Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Qin, Bosheng and Ye, Wentao and Yu, Qifan and Tang, Siliang and Zhuang, Yueting},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07749 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\TZ55D6IU\\Qin et al. - 2023 - Dancing Avatar Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\4F7IVGQ3\\2308.html:text/html},
}

@misc{lv_gpt4motion_2024,
	title = {{GPT4Motion}: {Scripting} {Physical} {Motions} in {Text}-to-{Video} {Generation} via {Blender}-{Oriented} {GPT} {Planning}},
	shorttitle = {{GPT4Motion}},
	url = {http://arxiv.org/abs/2311.12631},
	doi = {10.48550/arXiv.2311.12631},
	abstract = {Recent advances in text-to-video generation have harnessed the power of diffusion models to create visually compelling content conditioned on text prompts. However, they usually encounter high computational costs and often struggle to produce videos with coherent physical motions. To tackle these issues, we propose GPT4Motion, a training-free framework that leverages the planning capability of large language models such as GPT, the physical simulation strength of Blender, and the excellent image generation ability of text-to-image diffusion models to enhance the quality of video synthesis. Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a user textual prompt, which commands Blender's built-in physics engine to craft fundamental scene components that encapsulate coherent physical motions across frames. Then these components are inputted into Stable Diffusion to generate a video aligned with the textual prompt. Experimental results on three basic physical motion scenarios, including rigid object drop and collision, cloth draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate high-quality videos efficiently in maintaining motion coherency and entity consistency. GPT4Motion offers new insights in text-to-video research, enhancing its quality and broadening its horizon for further explorations.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Lv, Jiaxi and Huang, Yi and Yan, Mingfu and Huang, Jiancheng and Liu, Jianzhuang and Liu, Yifan and Wen, Yafei and Chen, Xiaoxin and Chen, Shifeng},
	month = apr,
	year = {2024},
	note = {arXiv:2311.12631 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\LCBETINM\\Lv et al. - 2024 - GPT4Motion Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\PFQGZL6Q\\2311.html:text/html},
}

@misc{bar-tal_lumiere_2024,
	title = {Lumiere: {A} {Space}-{Time} {Diffusion} {Model} for {Video} {Generation}},
	shorttitle = {Lumiere},
	url = {http://arxiv.org/abs/2401.12945},
	doi = {10.48550/arXiv.2401.12945},
	abstract = {We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann, Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel and Hur, Junhwa and Liu, Guanghui and Raj, Amit and Li, Yuanzhen and Rubinstein, Michael and Michaeli, Tomer and Wang, Oliver and Sun, Deqing and Dekel, Tali and Mosseri, Inbar},
	month = feb,
	year = {2024},
	note = {arXiv:2401.12945 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\7I469LUL\\Bar-Tal et al. - 2024 - Lumiere A Space-Time Diffusion Model for Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\7ZD6Z2KD\\2401.html:text/html},
}

@misc{chen_videodreamer_2025,
	title = {{VideoDreamer}: {Customized} {Multi}-{Subject} {Text}-to-{Video} {Generation} with {Disen}-{Mix} {Finetuning} on {Language}-{Video} {Foundation} {Models}},
	shorttitle = {{VideoDreamer}},
	url = {http://arxiv.org/abs/2311.00990},
	doi = {10.48550/arXiv.2311.00990},
	abstract = {Customized text-to-video generation aims to generate text-guided videos with user-given subjects, which has gained increasing attention. However, existing works are primarily limited to single-subject oriented text-to-video generation, leaving the more challenging problem of customized multi-subject generation unexplored. In this paper, we fill this gap and propose a novel VideoDreamer framework, which can generate temporally consistent text-guided videos that faithfully preserve the visual features of the given multiple subjects. Specifically, VideoDreamer adopts the pretrained Stable Diffusion with temporal modules as its base video generator, taking the power of the text-to-image model to generate diversified content. The video generator is further customized for multi-subjects, which leverages the proposed Disen-Mix Finetuning and Human-in-the-Loop Re-finetuning strategy, to tackle the attribute binding problem of multi-subject generation. Additionally, we present a disentangled motion customization strategy to finetune the temporal modules so that we can generate videos with both customized subjects and motions. To evaluate the performance of customized multi-subject text-to-video generation, we introduce the MultiStudioBench benchmark. Extensive experiments demonstrate the remarkable ability of VideoDreamer to generate videos with new content such as new events and backgrounds, tailored to the customized multiple subjects.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Chen, Hong and Wang, Xin and Zeng, Guanning and Zhang, Yipeng and Zhou, Yuwei and Han, Feilin and Wu, Yaofei and Zhu, Wenwu},
	month = apr,
	year = {2025},
	note = {arXiv:2311.00990 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\MGX9HV6S\\Chen et al. - 2025 - VideoDreamer Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning on Languag.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\8557AR3Z\\2311.html:text/html},
}

@misc{chen_videocrafter2_2024,
	title = {{VideoCrafter2}: {Overcoming} {Data} {Limitations} for {High}-{Quality} {Video} {Diffusion} {Models}},
	shorttitle = {{VideoCrafter2}},
	url = {https://arxiv.org/abs/2401.09047v1},
	abstract = {Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.},
	language = {en},
	urldate = {2025-06-02},
	journal = {arXiv.org},
	author = {Chen, Haoxin and Zhang, Yong and Cun, Xiaodong and Xia, Menghan and Wang, Xintao and Weng, Chao and Shan, Ying},
	month = jan,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\rysza\\Zotero\\storage\\ZK6ZJGBV\\Chen et al. - 2024 - VideoCrafter2 Overcoming Data Limitations for High-Quality Video Diffusion Models.pdf:application/pdf},
}

@misc{an_latent-shift_2023,
	title = {Latent-{Shift}: {Latent} {Diffusion} with {Temporal} {Shift} for {Efficient} {Text}-to-{Video} {Generation}},
	shorttitle = {Latent-{Shift}},
	url = {http://arxiv.org/abs/2304.08477},
	doi = {10.48550/arXiv.2304.08477},
	abstract = {We propose Latent-Shift -- an efficient text-to-video generation method based on a pretrained text-to-image generation model that consists of an autoencoder and a U-Net diffusion model. Learning a video diffusion model in the latent space is much more efficient than in the pixel space. The latter is often limited to first generating a low-resolution video followed by a sequence of frame interpolation and super-resolution models, which makes the entire pipeline very complex and computationally expensive. To extend a U-Net from image generation to video generation, prior work proposes to add additional modules like 1D temporal convolution and/or temporal attention layers. In contrast, we propose a parameter-free temporal shift module that can leverage the spatial U-Net as is for video generation. We achieve this by shifting two portions of the feature map channels forward and backward along the temporal dimension. The shifted features of the current frame thus receive the features from the previous and the subsequent frames, enabling motion learning without additional parameters. We show that Latent-Shift achieves comparable or better results while being significantly more efficient. Moreover, Latent-Shift can generate images despite being finetuned for T2V generation.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {An, Jie and Zhang, Songyang and Yang, Harry and Gupta, Sonal and Huang, Jia-Bin and Luo, Jiebo and Yin, Xi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08477 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\P8HS3BW8\\An et al. - 2023 - Latent-Shift Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\M6S2W7UC\\2304.html:text/html},
}

@misc{ge_preserve_2024,
	title = {Preserve {Your} {Own} {Correlation}: {A} {Noise} {Prior} for {Video} {Diffusion} {Models}},
	shorttitle = {Preserve {Your} {Own} {Correlation}},
	url = {http://arxiv.org/abs/2305.10474},
	doi = {10.48550/arXiv.2305.10474},
	abstract = {Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also achieves SOTA video generation quality on the small-scale UCF-101 benchmark with a \$10{\textbackslash}times\$ smaller model using significantly less computation than the prior art.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh},
	month = mar,
	year = {2024},
	note = {arXiv:2305.10474 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\ESNDS2ES\\Ge et al. - 2024 - Preserve Your Own Correlation A Noise Prior for Video Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\8FVJSV4K\\2305.html:text/html},
}

@misc{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	doi = {10.48550/arXiv.1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv:1610.02357 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\KMEGIHKU\\Chollet - 2017 - Xception Deep Learning with Depthwise Separable Convolutions.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\W7B9BAXG\\1610.html:text/html},
}

@misc{kumar_obamanet_2017,
	title = {{ObamaNet}: {Photo}-realistic lip-sync from text},
	shorttitle = {{ObamaNet}},
	url = {http://arxiv.org/abs/1801.01442},
	doi = {10.48550/arXiv.1801.01442},
	abstract = {We present ObamaNet, the first architecture that generates both audio and synchronized photo-realistic lip-sync videos from any new text. Contrary to other published lip-sync approaches, ours is only composed of fully trainable neural modules and does not rely on any traditional computer graphics methods. More precisely, we use three main modules: a text-to-speech network based on Char2Wav, a time-delayed LSTM to generate mouth-keypoints synced to the audio, and a network based on Pix2Pix to generate the video frames conditioned on the keypoints.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Kumar, Rithesh and Sotelo, Jose and Kumar, Kundan and Brebisson, Alexandre de and Bengio, Yoshua},
	month = dec,
	year = {2017},
	note = {arXiv:1801.01442 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\YX4AQNDM\\Kumar et al. - 2017 - ObamaNet Photo-realistic lip-sync from text.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\X4I3YGNF\\1801.html:text/html},
}

@inproceedings{mittal_sync-draw_2017,
	title = {Sync-{DRAW}: {Automatic} {Video} {Generation} using {Deep} {Recurrent} {Attentive} {Architectures}},
	shorttitle = {Sync-{DRAW}},
	url = {http://arxiv.org/abs/1611.10314},
	doi = {10.1145/3123266.3123309},
	abstract = {This paper introduces a novel approach for generating videos called Synchronized Deep Recurrent Attentive Writer (Sync-DRAW). Sync-DRAW can also perform text-to-video generation which, to the best of our knowledge, makes it the first approach of its kind. It combines a Variational Autoencoder{\textasciitilde}(VAE) with a Recurrent Attention Mechanism in a novel manner to create a temporally dependent sequence of frames that are gradually formed over time. The recurrent attention mechanism in Sync-DRAW attends to each individual frame of the video in sychronization, while the VAE learns a latent distribution for the entire video at the global level. Our experiments with Bouncing MNIST, KTH and UCF-101 suggest that Sync-DRAW is efficient in learning the spatial and temporal information of the videos and generates frames with high structural integrity, and can generate videos from simple captions on these datasets. (Accepted as oral paper in ACM-Multimedia 2017)},
	urldate = {2025-06-02},
	booktitle = {Proceedings of the 25th {ACM} international conference on {Multimedia}},
	author = {Mittal, Gaurav and Marwah, Tanya and Balasubramanian, Vineeth N.},
	month = oct,
	year = {2017},
	note = {arXiv:1611.10314 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1096--1104},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\X334UBCT\\Mittal et al. - 2017 - Sync-DRAW Automatic Video Generation using Deep Recurrent Attentive Architectures.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\EHRIX2BT\\1611.html:text/html},
}

@misc{lei_mart_2020,
	title = {{MART}: {Memory}-{Augmented} {Recurrent} {Transformer} for {Coherent} {Video} {Paragraph} {Captioning}},
	shorttitle = {{MART}},
	url = {http://arxiv.org/abs/2005.05402},
	doi = {10.48550/arXiv.2005.05402},
	abstract = {Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events. All code is available open-source at: https://github.com/jayleicn/recurrent-transformer},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Lei, Jie and Wang, Liwei and Shen, Yelong and Yu, Dong and Berg, Tamara L. and Bansal, Mohit},
	month = may,
	year = {2020},
	note = {arXiv:2005.05402 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\VHT5AP77\\Lei et al. - 2020 - MART Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\9RQ3PN2T\\2005.html:text/html},
}

@article{kim_tivgan_2020,
	title = {{TiVGAN}: {Text} to {Image} to {Video} {Generation} {With} {Step}-by-{Step} {Evolutionary} {Generator}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{TiVGAN}},
	url = {https://ieeexplore.ieee.org/document/9171240},
	doi = {10.1109/ACCESS.2020.3017881},
	abstract = {Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames. This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.},
	urldate = {2025-06-02},
	journal = {IEEE Access},
	author = {Kim, Doyeon and Joo, Donggyu and Kim, Junmo},
	year = {2020},
	keywords = {Computer vision, deep learning, Gallium nitride, generative adversarial networks, Generative adversarial networks, Generators, Image synthesis, Streaming media, Task analysis, text-to-video generation, Training, video generation},
	pages = {153113--153122},
	file = {Full Text PDF:C\:\\Users\\rysza\\Zotero\\storage\\9QRITDXQ\\Kim et al. - 2020 - TiVGAN Text to Image to Video Generation With Step-by-Step Evolutionary Generator.pdf:application/pdf},
}

@article{noauthor_pdf_2025,
	title = {({PDF}) {Long} {Short}-{Term} {Memory}},
	url = {https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {PDF {\textbar} Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient,... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-06-02},
	journal = {ResearchGate},
	month = feb,
	year = {2025},
}

@misc{jiang_text2performer_2023,
	title = {{Text2Performer}: {Text}-{Driven} {Human} {Video} {Generation}},
	shorttitle = {{Text2Performer}},
	url = {http://arxiv.org/abs/2304.08483},
	doi = {10.48550/arXiv.2304.08483},
	abstract = {Text-driven content creation has evolved to be a transformative technique that revolutionizes creativity. Here we study the task of text-driven human video generation, where a video sequence is synthesized from texts describing the appearance and motions of a target performer. Compared to general text-driven video generation, human-centric video generation requires maintaining the appearance of synthesized human while performing complex motions. In this work, we present Text2Performer to generate vivid human videos with articulated motions from texts. Text2Performer has two novel designs: 1) decomposed human representation and 2) diffusion-based motion sampler. First, we decompose the VQVAE latent space into human appearance and pose representation in an unsupervised manner by utilizing the nature of human videos. In this way, the appearance is well maintained along the generated frames. Then, we propose continuous VQ-diffuser to sample a sequence of pose embeddings. Unlike existing VQ-based methods that operate in the discrete space, continuous VQ-diffuser directly outputs the continuous pose embeddings for better motion modeling. Finally, motion-aware masking strategy is designed to mask the pose embeddings spatial-temporally to enhance the temporal coherence. Moreover, to facilitate the task of text-driven human video generation, we contribute a Fashion-Text2Video dataset with manually annotated action labels and text descriptions. Extensive experiments demonstrate that Text2Performer generates high-quality human videos (up to 512x256 resolution) with diverse appearances and flexible motions.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Jiang, Yuming and Yang, Shuai and Koh, Tong Liang and Wu, Wayne and Loy, Chen Change and Liu, Ziwei},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08483 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\3LWBGD22\\Jiang et al. - 2023 - Text2Performer Text-Driven Human Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\VQQSWCA3\\2304.html:text/html},
}

@misc{wu_godiva_2021,
	title = {{GODIVA}: {Generating} {Open}-{DomaIn} {Videos} from {nAtural} {Descriptions}},
	shorttitle = {{GODIVA}},
	url = {http://arxiv.org/abs/2104.14806},
	doi = {10.48550/arXiv.2104.14806},
	abstract = {Generating videos from text is a challenging task due to its high computational requirements for training and infinite possible answers for evaluation. Existing works typically experiment on simple or small datasets, where the generalization ability is quite limited. In this work, we propose GODIVA, an open-domain text-to-video pretrained model that can generate videos from text in an auto-regressive manner using a three-dimensional sparse attention mechanism. We pretrain our model on Howto100M, a large-scale text-video dataset that contains more than 136 million text-video pairs. Experiments show that GODIVA not only can be fine-tuned on downstream video generation tasks, but also has a good zero-shot capability on unseen texts. We also propose a new metric called Relative Matching (RM) to automatically evaluate the video generation quality. Several challenges are listed and discussed as future work.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Wu, Chenfei and Huang, Lun and Zhang, Qianxi and Li, Binyang and Ji, Lei and Yang, Fan and Sapiro, Guillermo and Duan, Nan},
	month = apr,
	year = {2021},
	note = {arXiv:2104.14806 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\V7V69K92\\Wu et al. - 2021 - GODIVA Generating Open-DomaIn Videos from nAtural Descriptions.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\EN2LQFMB\\2104.html:text/html},
}

@misc{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv:1806.07366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\4F3RN5XT\\Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\59G5DP3I\\1806.html:text/html},
}

@misc{ahn_story_2023,
	title = {Story {Visualization} by {Online} {Text} {Augmentation} with {Context} {Memory}},
	url = {http://arxiv.org/abs/2308.07575},
	doi = {10.48550/arXiv.2308.07575},
	abstract = {Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Ahn, Daechul and Kim, Daneul and Song, Gwangmo and Kim, Seung Hwan and Lee, Honglak and Kang, Dongyeop and Choi, Jonghyun},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07575 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\9RSNLH29\\Ahn et al. - 2023 - Story Visualization by Online Text Augmentation with Context Memory.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\TK3NYABI\\2308.html:text/html},
}

@misc{gupta_photorealistic_2023,
	title = {Photorealistic {Video} {Generation} with {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2312.06662},
	doi = {10.48550/arXiv.2312.06662},
	abstract = {We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of \$512 {\textbackslash}times 896\$ resolution at \$8\$ frames per second.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Gupta, Agrim and Yu, Lijun and Sohn, Kihyuk and Gu, Xiuye and Hahn, Meera and Fei-Fei, Li and Essa, Irfan and Jiang, Lu and Lezama, José},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06662 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\E65KSVLM\\Gupta et al. - 2023 - Photorealistic Video Generation with Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\HIBSIPLH\\2312.html:text/html},
}

@misc{hu_make_2022,
	title = {Make {It} {Move}: {Controllable} {Image}-to-{Video} {Generation} with {Text} {Descriptions}},
	shorttitle = {Make {It} {Move}},
	url = {http://arxiv.org/abs/2112.02815},
	doi = {10.48550/arXiv.2112.02815},
	abstract = {Generating controllable videos conforming to user intentions is an appealing yet challenging topic in computer vision. To enable maneuverable control in line with user intentions, a novel video generation task, named Text-Image-to-Video generation (TI2V), is proposed. With both controllable appearance and motion, TI2V aims at generating videos from a static image and a text description. The key challenges of TI2V task lie both in aligning appearance and motion from different modalities, and in handling uncertainty in text descriptions. To address these challenges, we propose a Motion Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA) structure to store appearance-motion aligned representation. To model the uncertainty and increase the diversity, it further allows the injection of explicit condition and implicit randomness. Through three-dimensional axial transformers, MA is interacted with given image to generate next frames recursively with satisfying controllability and diversity. Accompanying the new task, we build two new video-text paired datasets based on MNIST and CATER for evaluation. Experiments conducted on these datasets verify the effectiveness of MAGE and show appealing potentials of TI2V task. Source code for model and datasets will be available soon.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Hu, Yaosi and Luo, Chong and Chen, Zhenzhong},
	month = mar,
	year = {2022},
	note = {arXiv:2112.02815 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\9IZFGZDB\\Hu et al. - 2022 - Make It Move Controllable Image-to-Video Generation with Text Descriptions.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\XI6I698E\\2112.html:text/html},
}

@misc{yin_nuwa-xl_2023,
	title = {{NUWA}-{XL}: {Diffusion} over {Diffusion} for {eXtremely} {Long} {Video} {Generation}},
	shorttitle = {{NUWA}-{XL}},
	url = {http://arxiv.org/abs/2303.12346},
	doi = {10.48550/arXiv.2303.12346},
	abstract = {In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ``coarse-to-fine'' process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26{\textbackslash}\%) at the same hardware setting when generating 1024 frames. The homepage link is {\textbackslash}url\{https://msra-nuwa.azurewebsites.net/\}},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Yin, Shengming and Wu, Chenfei and Yang, Huan and Wang, Jianfeng and Wang, Xiaodong and Ni, Minheng and Yang, Zhengyuan and Li, Linjie and Liu, Shuguang and Yang, Fan and Fu, Jianlong and Ming, Gong and Wang, Lijuan and Liu, Zicheng and Li, Houqiang and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12346 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\GW89GQ4B\\Yin et al. - 2023 - NUWA-XL Diffusion over Diffusion for eXtremely Long Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\XJ3CHUL4\\2303.html:text/html},
}

@misc{fei_dysen-vdm_2024,
	title = {Dysen-{VDM}: {Empowering} {Dynamics}-aware {Text}-to-{Video} {Diffusion} with {LLMs}},
	shorttitle = {Dysen-{VDM}},
	url = {http://arxiv.org/abs/2308.13812},
	doi = {10.48550/arXiv.2308.13812},
	abstract = {Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via in-context learning, Dysen realizes (nearly) human-level temporal dynamics understanding. Finally, the resulting video DSG with rich action scene details is encoded as fine-grained spatio-temporal features, integrated into the backbone T2V DM for video generating. Experiments on popular T2V datasets suggest that our Dysen-VDM consistently outperforms prior arts with significant margins, especially in scenarios with complex actions. Codes at https://haofei.vip/Dysen-VDM},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Fei, Hao and Wu, Shengqiong and Ji, Wei and Zhang, Hanwang and Chua, Tat-Seng},
	month = mar,
	year = {2024},
	note = {arXiv:2308.13812 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\HPMU6LA8\\Fei et al. - 2024 - Dysen-VDM Empowering Dynamics-aware Text-to-Video Diffusion with LLMs.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\5KAY3L9R\\2308.html:text/html},
}

@misc{lv_gpt4motion_2024-1,
	title = {{GPT4Motion}: {Scripting} {Physical} {Motions} in {Text}-to-{Video} {Generation} via {Blender}-{Oriented} {GPT} {Planning}},
	shorttitle = {{GPT4Motion}},
	url = {http://arxiv.org/abs/2311.12631},
	doi = {10.48550/arXiv.2311.12631},
	abstract = {Recent advances in text-to-video generation have harnessed the power of diffusion models to create visually compelling content conditioned on text prompts. However, they usually encounter high computational costs and often struggle to produce videos with coherent physical motions. To tackle these issues, we propose GPT4Motion, a training-free framework that leverages the planning capability of large language models such as GPT, the physical simulation strength of Blender, and the excellent image generation ability of text-to-image diffusion models to enhance the quality of video synthesis. Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a user textual prompt, which commands Blender's built-in physics engine to craft fundamental scene components that encapsulate coherent physical motions across frames. Then these components are inputted into Stable Diffusion to generate a video aligned with the textual prompt. Experimental results on three basic physical motion scenarios, including rigid object drop and collision, cloth draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate high-quality videos efficiently in maintaining motion coherency and entity consistency. GPT4Motion offers new insights in text-to-video research, enhancing its quality and broadening its horizon for further explorations.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Lv, Jiaxi and Huang, Yi and Yan, Mingfu and Huang, Jiancheng and Liu, Jianzhuang and Liu, Yifan and Wen, Yafei and Chen, Xiaoxin and Chen, Shifeng},
	month = apr,
	year = {2024},
	note = {arXiv:2311.12631 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\FUSSDHDR\\Lv et al. - 2024 - GPT4Motion Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\BZ2FE93M\\2311.html:text/html},
}

@misc{peebles_scalable_2023,
	title = {Scalable {Diffusion} {Models} with {Transformers}},
	url = {http://arxiv.org/abs/2212.09748},
	doi = {10.48550/arXiv.2212.09748},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Peebles, William and Xie, Saining},
	month = mar,
	year = {2023},
	note = {arXiv:2212.09748 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\P5455S38\\Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\VFI5CXPP\\2212.html:text/html},
}

@misc{arnab_vivit_2021,
	title = {{ViViT}: {A} {Video} {Vision} {Transformer}},
	shorttitle = {{ViViT}},
	url = {http://arxiv.org/abs/2103.15691},
	doi = {10.48550/arXiv.2103.15691},
	abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
	month = nov,
	year = {2021},
	note = {arXiv:2103.15691 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\4XYPV7FE\\Arnab et al. - 2021 - ViViT A Video Vision Transformer.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\XNCNKJJZ\\2103.html:text/html},
}

@misc{maharana_storydall-e_2022,
	title = {{StoryDALL}-{E}: {Adapting} {Pretrained} {Text}-to-{Image} {Transformers} for {Story} {Continuation}},
	shorttitle = {{StoryDALL}-{E}},
	url = {http://arxiv.org/abs/2209.06192},
	doi = {10.48550/arXiv.2209.06192},
	abstract = {Recent advances in text-to-image synthesis have led to large pretrained transformers with excellent capabilities to generate visualizations from a given text. However, these models are ill-suited for specialized tasks like story visualization, which requires an agent to produce a sequence of images given a corresponding sequence of captions, forming a narrative. Moreover, we find that the story visualization task fails to accommodate generalization to unseen plots and characters in new narratives. Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters. Then, we enhance or 'retro-fit' the pretrained text-to-image synthesis models with task-specific modules for (a) sequential image generation and (b) copying relevant elements from an initial frame. Then, we explore full-model finetuning, as well as prompt-based tuning for parameter-efficient adaptation, of the pre-trained model. We evaluate our approach StoryDALL-E on two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset DiDeMoSV collected from a video-captioning dataset. We also develop a model StoryGANc based on Generative Adversarial Networks (GAN) for story continuation, and compare it with the StoryDALL-E model to demonstrate the advantages of our approach. We show that our retro-fitting approach outperforms GAN-based models for story continuation and facilitates copying of visual elements from the source image, thereby improving continuity in the generated visual story. Finally, our analysis suggests that pretrained transformers struggle to comprehend narratives containing several characters. Overall, our work demonstrates that pretrained text-to-image synthesis models can be adapted for complex and low-resource tasks like story continuation.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Maharana, Adyasha and Hannan, Darryl and Bansal, Mohit},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06192 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\QT2URR67\\Maharana et al. - 2022 - StoryDALL-E Adapting Pretrained Text-to-Image Transformers for Story Continuation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\SLXIB64P\\2209.html:text/html},
}

@misc{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	doi = {10.48550/arXiv.1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv:1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\5Q8AZXY9\\Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\YGUZH632\\1711.html:text/html},
}

@misc{li_storygan_2019,
	title = {{StoryGAN}: {A} {Sequential} {Conditional} {GAN} for {Story} {Visualization}},
	shorttitle = {{StoryGAN}},
	url = {http://arxiv.org/abs/1812.02784},
	doi = {10.48550/arXiv.1812.02784},
	abstract = {We propose a new task, called Story Visualization. Given a multi-sentence paragraph, the story is visualized by generating a sequence of images, one for each sentence. In contrast to video generation, story visualization focuses less on the continuity in generated images (frames), but more on the global consistency across dynamic scenes and characters -- a challenge that has not been addressed by any single-image or video generation methods. We therefore propose a new story-to-image-sequence generation model, StoryGAN, based on the sequential conditional GAN framework. Our model is unique in that it consists of a deep Context Encoder that dynamically tracks the story flow, and two discriminators at the story and image levels, to enhance the image quality and the consistency of the generated sequences. To evaluate the model, we modified existing datasets to create the CLEVR-SV and Pororo-SV datasets. Empirically, StoryGAN outperforms state-of-the-art models in image quality, contextual consistency metrics, and human evaluation.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Li, Yitong and Gan, Zhe and Shen, Yelong and Liu, Jingjing and Cheng, Yu and Wu, Yuexin and Carin, Lawrence and Carlson, David and Gao, Jianfeng},
	month = apr,
	year = {2019},
	note = {arXiv:1812.02784 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\2GH9EXLX\\Li et al. - 2019 - StoryGAN A Sequential Conditional GAN for Story Visualization.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\3CZ3FV4Z\\1812.html:text/html},
}

@misc{blattmann_stable_2023,
	title = {Stable {Video} {Diffusion}: {Scaling} {Latent} {Video} {Diffusion} {Models} to {Large} {Datasets}},
	shorttitle = {Stable {Video} {Diffusion}},
	url = {http://arxiv.org/abs/2311.15127},
	doi = {10.48550/arXiv.2311.15127},
	abstract = {We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin},
	month = nov,
	year = {2023},
	note = {arXiv:2311.15127 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\9LPHEV5L\\Blattmann et al. - 2023 - Stable Video Diffusion Scaling Latent Video Diffusion Models to Large Datasets.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\SPV7HVUU\\2311.html:text/html},
}

@misc{blattmann_align_2023,
	title = {Align your {Latents}: {High}-{Resolution} {Video} {Synthesis} with {Latent} {Diffusion} {Models}},
	shorttitle = {Align your {Latents}},
	url = {http://arxiv.org/abs/2304.08818},
	doi = {10.48550/arXiv.2304.08818},
	abstract = {Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://research.nvidia.com/labs/toronto-ai/VideoLDM/},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
	month = dec,
	year = {2023},
	note = {arXiv:2304.08818 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\A8MT6SQU\\Blattmann et al. - 2023 - Align your Latents High-Resolution Video Synthesis with Latent Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\WNEZ6S8Z\\2304.html:text/html},
}

@misc{guo_animatediff_2024,
	title = {{AnimateDiff}: {Animate} {Your} {Personalized} {Text}-to-{Image} {Diffusion} {Models} without {Specific} {Tuning}},
	shorttitle = {{AnimateDiff}},
	url = {http://arxiv.org/abs/2307.04725},
	doi = {10.48550/arXiv.2307.04725},
	abstract = {With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
	month = feb,
	year = {2024},
	note = {arXiv:2307.04725 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\75X3ACZU\\Guo et al. - 2024 - AnimateDiff Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\ZCX87ZRI\\2307.html:text/html},
}

@misc{wu_tune--video_2023,
	title = {Tune-{A}-{Video}: {One}-{Shot} {Tuning} of {Image} {Diffusion} {Models} for {Text}-to-{Video} {Generation}},
	shorttitle = {Tune-{A}-{Video}},
	url = {http://arxiv.org/abs/2212.11565},
	doi = {10.48550/arXiv.2212.11565},
	abstract = {To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting\${\textbackslash}unicode\{x2014\}\$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
	month = mar,
	year = {2023},
	note = {arXiv:2212.11565 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\5N2SRZ5N\\Wu et al. - 2023 - Tune-A-Video One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\C4KXK3BU\\2212.html:text/html},
}

@misc{ma_follow_2024,
	title = {Follow {Your} {Pose}: {Pose}-{Guided} {Text}-to-{Video} {Generation} using {Pose}-{Free} {Videos}},
	shorttitle = {Follow {Your} {Pose}},
	url = {http://arxiv.org/abs/2304.01186},
	doi = {10.48550/arXiv.2304.01186},
	abstract = {Generating text-editable and pose-controllable character videos have an imperious demand in creating various digital human. Nevertheless, this task has been restricted by the absence of a comprehensive dataset featuring paired video-pose captions and the generative prior models for videos. In this work, we design a novel two-stage training scheme that can utilize easily obtained datasets (i.e.,image pose pair and pose-free video) and the pre-trained text-to-image (T2I) model to obtain the pose-controllable character videos. Specifically, in the first stage, only the keypoint-image pairs are used only for a controllable text-to-image generation. We learn a zero-initialized convolutional encoder to encode the pose information. In the second stage, we finetune the motion of the above network via a pose-free video dataset by adding the learnable temporal self-attention and reformed cross-frame self-attention blocks. Powered by our new designs, our method successfully generates continuously pose-controllable character videos while keeps the editing and concept composition ability of the pre-trained T2I model. The code and models will be made publicly available.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Ma, Yue and He, Yingqing and Cun, Xiaodong and Wang, Xintao and Chen, Siran and Shan, Ying and Li, Xiu and Chen, Qifeng},
	month = jan,
	year = {2024},
	note = {arXiv:2304.01186 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\7G6T7VYP\\Ma et al. - 2024 - Follow Your Pose Pose-Guided Text-to-Video Generation using Pose-Free Videos.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\PHQMRIVP\\2304.html:text/html},
}

@misc{kondratyuk_videopoet_2024,
	title = {{VideoPoet}: {A} {Large} {Language} {Model} for {Zero}-{Shot} {Video} {Generation}},
	shorttitle = {{VideoPoet}},
	url = {http://arxiv.org/abs/2312.14125},
	doi = {10.48550/arXiv.2312.14125},
	abstract = {We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, José and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and Somandepalli, Krishna and Akbari, Hassan and Alon, Yair and Cheng, Yong and Dillon, Josh and Gupta, Agrim and Hahn, Meera and Hauth, Anja and Hendon, David and Martinez, Alonso and Minnen, David and Sirotenko, Mikhail and Sohn, Kihyuk and Yang, Xuan and Adam, Hartwig and Yang, Ming-Hsuan and Essa, Irfan and Wang, Huisheng and Ross, David A. and Seybold, Bryan and Jiang, Lu},
	month = jun,
	year = {2024},
	note = {arXiv:2312.14125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\PIQNATHD\\Kondratyuk et al. - 2024 - VideoPoet A Large Language Model for Zero-Shot Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\654VV6DW\\2312.html:text/html},
}

@misc{hong_cogvideo_2022,
	title = {{CogVideo}: {Large}-scale {Pretraining} for {Text}-to-{Video} {Generation} via {Transformers}},
	shorttitle = {{CogVideo}},
	url = {http://arxiv.org/abs/2205.15868},
	doi = {10.48550/arXiv.2205.15868},
	abstract = {Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
	month = may,
	year = {2022},
	note = {arXiv:2205.15868 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\T5NCFY2C\\Hong et al. - 2022 - CogVideo Large-scale Pretraining for Text-to-Video Generation via Transformers.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\D2C73TH8\\2205.html:text/html},
}

@misc{villegas_phenaki_2022,
	title = {Phenaki: {Variable} {Length} {Video} {Generation} {From} {Open} {Domain} {Textual} {Description}},
	shorttitle = {Phenaki},
	url = {http://arxiv.org/abs/2210.02399},
	doi = {10.48550/arXiv.2210.02399},
	abstract = {We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Villegas, Ruben and Babaeizadeh, Mohammad and Kindermans, Pieter-Jan and Moraldo, Hernan and Zhang, Han and Saffar, Mohammad Taghi and Castro, Santiago and Kunze, Julius and Erhan, Dumitru},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02399 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\VB7XM8PQ\\Villegas et al. - 2022 - Phenaki Variable Length Video Generation From Open Domain Textual Description.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\PFHV9RLM\\2210.html:text/html},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	language = {en},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
}

@misc{kim_tivgan_2021,
	title = {{TiVGAN}: {Text} to {Image} to {Video} {Generation} with {Step}-by-{Step} {Evolutionary} {Generator}},
	shorttitle = {{TiVGAN}},
	url = {http://arxiv.org/abs/2009.02018},
	doi = {10.48550/arXiv.2009.02018},
	abstract = {Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames.This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Kim, Doyeon and Joo, Donggyu and Kim, Junmo},
	month = jun,
	year = {2021},
	note = {arXiv:2009.02018 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\P7WQYMET\\Kim et al. - 2021 - TiVGAN Text to Image to Video Generation with Step-by-Step Evolutionary Generator.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\JXWZR3M4\\2009.html:text/html},
}

@misc{li_video_2017,
	title = {Video {Generation} {From} {Text}},
	url = {http://arxiv.org/abs/1710.00421},
	doi = {10.48550/arXiv.1710.00421},
	abstract = {Generating videos from text has proven to be a significant challenge for existing generative models. We tackle this problem by training a conditional generative model to extract both static and dynamic information from text. This is manifested in a hybrid framework, employing a Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN). The static features, called "gist," are used to sketch text-conditioned background color and object layout structure. Dynamic features are considered by transforming input text into an image filter. To obtain a large amount of data for training the deep-learning model, we develop a method to automatically create a matched text-video corpus from publicly available online videos. Experimental results show that the proposed framework generates plausible and diverse videos, while accurately reflecting the input text information. It significantly outperforms baseline models that directly adapt text-to-image generation procedures to produce videos. Performance is evaluated both visually and by adapting the inception score used to evaluate image generation in GANs.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Li, Yitong and Min, Martin Renqiang and Shen, Dinghan and Carlson, David and Carin, Lawrence},
	month = oct,
	year = {2017},
	note = {arXiv:1710.00421 [cs]},
	keywords = {Computer Science - Multimedia},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\2JCH23KS\\Li et al. - 2017 - Video Generation From Text.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\DIR2X7UE\\1710.html:text/html},
}

@misc{pan_create_2018,
	title = {To {Create} {What} {You} {Tell}: {Generating} {Videos} from {Captions}},
	shorttitle = {To {Create} {What} {You} {Tell}},
	url = {http://arxiv.org/abs/1804.08264},
	doi = {10.48550/arXiv.1804.08264},
	abstract = {We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Pan, Yingwei and Qiu, Zhaofan and Yao, Ting and Li, Houqiang and Mei, Tao},
	month = apr,
	year = {2018},
	note = {arXiv:1804.08264 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\2IHY5QI7\\Pan et al. - 2018 - To Create What You Tell Generating Videos from Captions.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\JMRHFWFF\\1804.html:text/html},
}

@misc{kiros_skip-thought_2015,
	title = {Skip-{Thought} {Vectors}},
	url = {http://arxiv.org/abs/1506.06726},
	doi = {10.48550/arXiv.1506.06726},
	abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	month = jun,
	year = {2015},
	note = {arXiv:1506.06726 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\FCAGRYMD\\Kiros et al. - 2015 - Skip-Thought Vectors.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\MZ448X64\\1506.html:text/html},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2025-06-02},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\QEQMZK48\\Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{singer_make--video_2022,
	title = {Make-{A}-{Video}: {Text}-to-{Video} {Generation} without {Text}-{Video} {Data}},
	shorttitle = {Make-{A}-{Video}},
	url = {http://arxiv.org/abs/2209.14792},
	doi = {10.48550/arXiv.2209.14792},
	abstract = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14792 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\IRI8GVJV\\Singer et al. - 2022 - Make-A-Video Text-to-Video Generation without Text-Video Data.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\4PSZPHDA\\2209.html:text/html},
}

@misc{song_clip_2022,
	title = {{CLIP} {Models} are {Few}-shot {Learners}: {Empirical} {Studies} on {VQA} and {Visual} {Entailment}},
	shorttitle = {{CLIP} {Models} are {Few}-shot {Learners}},
	url = {http://arxiv.org/abs/2203.07190},
	doi = {10.48550/arXiv.2203.07190},
	abstract = {CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIP's zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.},
	urldate = {2025-05-31},
	publisher = {arXiv},
	author = {Song, Haoyu and Dong, Li and Zhang, Wei-Nan and Liu, Ting and Wei, Furu},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07190 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\F9DULF3K\\Song et al. - 2022 - CLIP Models are Few-shot Learners Empirical Studies on VQA and Visual Entailment.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\GBTG5JTD\\2203.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-05-31},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\KU642DHR\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\FN9KSS9B\\1810.html:text/html},
}

@misc{blattmann_stable_2023-1,
	title = {Stable {Video} {Diffusion}: {Scaling} {Latent} {Video} {Diffusion} {Models} to {Large} {Datasets}},
	shorttitle = {Stable {Video} {Diffusion}},
	url = {http://arxiv.org/abs/2311.15127},
	doi = {10.48550/arXiv.2311.15127},
	abstract = {We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin},
	month = nov,
	year = {2023},
	note = {arXiv:2311.15127 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\7XUV4EYN\\Blattmann et al. - 2023 - Stable Video Diffusion Scaling Latent Video Diffusion Models to Large Datasets.pdf:application/pdf},
}

@misc{wang_swap_2024,
	title = {Swap {Attention} in {Spatiotemporal} {Diffusions} for {Text}-to-{Video} {Generation}},
	url = {http://arxiv.org/abs/2305.10874},
	doi = {10.48550/arXiv.2305.10874},
	abstract = {With the explosive popularity of AI-generated content (AIGC), video generation has recently received a lot of attention. Generating videos guided by text instructions poses significant challenges, such as modeling the complex relationship between space and time, and the lack of large-scale text-video paired data. Existing text-video datasets suffer from limitations in both content quality and scale, or they are not open-source, rendering them inaccessible for study and use. For model design, previous approaches extend pretrained text-to-image generation models by adding temporal 1D convolution/attention modules for video generation. However, these approaches overlook the importance of jointly modeling space and time, inevitably leading to temporal distortions and misalignment between texts and videos. In this paper, we propose a novel approach that strengthens the interaction between spatial and temporal perceptions. In particular, we utilize a swapped cross-attention mechanism in 3D windows that alternates the "query" role between spatial and temporal blocks, enabling mutual reinforcement for each other. Moreover, to fully unlock model capabilities for high-quality video generation and promote the development of the field, we curate a large-scale and open-source video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters. A smaller-scale yet more meticulously cleaned subset further enhances the data quality, aiding models in achieving superior performance. Experimental quantitative and qualitative results demonstrate the superiority of our approach in terms of per-frame quality, temporal correlation, and text-video alignment, with clear margins.},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Wang, Wenjing and Yang, Huan and Tuo, Zixi and He, Huiguo and Zhu, Junchen and Fu, Jianlong and Liu, Jiaying},
	month = apr,
	year = {2024},
	note = {arXiv:2305.10874 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\ULEGQZS5\\Wang et al. - 2024 - Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\M525Q9PK\\2305.html:text/html},
}

@misc{noauthor_veo_nodate,
	title = {Veo},
	url = {https://deepmind.google/models/veo/},
	abstract = {Introducing our state of the art video generation model Veo 3, and new capabilities for Veo 2.},
	language = {en},
	urldate = {2025-05-30},
	journal = {Google DeepMind},
	file = {Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\997UAZCJ\\veo.html:text/html},
}

@misc{noauthor_sora_nodate,
	title = {Sora: {Creating} video from text},
	shorttitle = {Sora},
	url = {https://openai.com/index/sora/},
	language = {en-US},
	urldate = {2025-05-30},
	file = {Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\ZMAWHWLH\\sora.html:text/html},
}

@misc{ho_imagen_2022,
	title = {Imagen {Video}: {High} {Definition} {Video} {Generation} with {Diffusion} {Models}},
	shorttitle = {Imagen {Video}},
	url = {http://arxiv.org/abs/2210.02303},
	doi = {10.48550/arXiv.2210.02303},
	abstract = {We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02303 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\ARYSN6AA\\Ho et al. - 2022 - Imagen Video High Definition Video Generation with Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\7DKWBFYE\\2210.html:text/html},
}

@misc{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = oct,
	year = {2014},
	note = {arXiv:1409.0473 [cs]
version: 3},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\JG726486\\Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\HLVLHWR2\\1409.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]
version: 5},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\FDRIPBIK\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\CZP5R5PQ\\1706.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\BSZFDQCX\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\DE7U5PTP\\2006.html:text/html},
}

@misc{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv:1312.6114 [stat]
version: 10},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\CN2KERNE\\Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\X69J4H2F\\1312.html:text/html},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\SYVRVGQE\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\YSN9XEU5\\2103.html:text/html},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	doi = {10.48550/arXiv.1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\5MCFRD6V\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\WH7YVVDT\\1609.html:text/html},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {http://arxiv.org/abs/2209.14988},
	doi = {10.48550/arXiv.2209.14988},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14988 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\5LTLAMRK\\Poole et al. - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\8WWP2G9F\\2209.html:text/html},
}

@misc{dhariwal_jukebox_2020,
	title = {Jukebox: {A} {Generative} {Model} for {Music}},
	shorttitle = {Jukebox},
	url = {http://arxiv.org/abs/2005.00341},
	doi = {10.48550/arXiv.2005.00341},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	month = apr,
	year = {2020},
	note = {arXiv:2005.00341 [eess]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\YRMC6ITY\\Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\BGFIGUUE\\2005.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\GID5G7LS\\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\2Q38SCGZ\\1503.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\rysza\\Zotero\\storage\\EI64YPHQ\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\LNUZGQVC\\1406.html:text/html},
}

@article{turing_computing_1950,
	title = {Computing {Machinery} and {Intelligence}},
	volume = {49},
	journal = {Mind},
	author = {Turing, A. M.},
	year = {1950},
	pages = {433--460},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\JHRJ86DQ\\Turing - 1950 - Computing Machinery and Intelligence.pdf:application/pdf},
}

@misc{jones_large_2025,
	title = {Large {Language} {Models} {Pass} the {Turing} {Test}},
	url = {http://arxiv.org/abs/2503.23674},
	doi = {10.48550/arXiv.2503.23674},
	abstract = {We evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomised, controlled, and pre-registered Turing tests on independent populations. Participants had 5 minute conversations simultaneously with another human participant and one of these systems before judging which conversational partner they thought was human. When prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73\% of the time: significantly more often than interrogators selected the real human participant. LLaMa-3.1, with the same prompt, was judged to be the human 56\% of the time—not significantly more or less often than the humans they were being compared to—while baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23\% and 21\% respectively). The results constitute the first empirical evidence that any artificial system passes a standard three-party Turing test. The results have implications for debates about what kind of intelligence is exhibited by Large Language Models (LLMs), and the social and economic impacts these systems are likely to have.},
	language = {en},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Jones, Cameron R. and Bergen, Benjamin K.},
	month = mar,
	year = {2025},
	note = {arXiv:2503.23674 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\VZX8RRQ4\\Jones and Bergen - 2025 - Large Language Models Pass the Turing Test.pdf:application/pdf},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and ﬂexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the ﬁrst time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual ﬁdelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and ﬂexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while signiﬁcantly reducing computational requirements compared to pixel-based DMs.},
	language = {en},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\R8SRMWKF\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf},
}

@misc{liu_sora_2024,
	title = {Sora: {A} {Review} on {Background}, {Technology}, {Limitations}, and {Opportunities} of {Large} {Vision} {Models}},
	shorttitle = {Sora},
	url = {http://arxiv.org/abs/2402.17177},
	doi = {10.48550/arXiv.2402.17177},
	abstract = {Note: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model’s background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora’s development and investigate the underlying technologies used to build this “world simulator”. Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.},
	language = {en},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and He, Lifang and Sun, Lichao},
	month = apr,
	year = {2024},
	note = {arXiv:2402.17177 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\QSK72MEP\\Liu et al. - 2024 - Sora A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models.pdf:application/pdf},
}

@misc{cho_sora_2024,
	title = {Sora as an {AGI} {World} {Model}? {A} {Complete} {Survey} on {Text}-to-{Video} {Generation}},
	shorttitle = {Sora as an {AGI} {World} {Model}?},
	url = {http://arxiv.org/abs/2403.05131},
	doi = {10.48550/arXiv.2403.05131},
	abstract = {The evolution of video generation from text, starting with animating MNIST numbers to simulating the physical world with Sora, has progressed at a breakneck speed over the past seven years. While often seen as a superficial expansion of the predecessor text-to-image generation model, text-to-video generation models are developed upon carefully engineered constituents. Here, we systematically discuss these elements consisting of but not limited to core building blocks (vision, language, and temporal) and supporting features from the perspective of their contributions to achieving a world model. We employ the PRISMA framework to curate 97 impactful research articles from renowned scientific databases primarily studying video synthesis using text conditions. Upon minute exploration of these manuscripts, we observe that text-to-video generation involves more intricate technologies beyond the plain extension of text-to-image generation. Our additional review into the shortcomings of Sora-generated videos pinpoints the call for more in-depth studies in various enabling aspects of video generation such as dataset, evaluation metric, efficient architecture, and human-controlled generation. Finally, we conclude that the study of the text-to-video generation may still be in its infancy, requiring contribution from the cross-discipline research community towards its advancement as the first step to realize artificial general intelligence (AGI).},
	language = {en},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Cho, Joseph and Puspitasari, Fachrina Dewi and Zheng, Sheng and Zheng, Jingyao and Lee, Lik-Hang and Kim, Tae-Ho and Hong, Choong Seon and Zhang, Chaoning},
	month = jun,
	year = {2024},
	note = {arXiv:2403.05131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:C\:\\Users\\rysza\\Zotero\\storage\\UH5NBLX3\\Cho et al. - 2024 - Sora as an AGI World Model A Complete Survey on Text-to-Video Generation.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	urldate = {2025-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:C\:\\Users\\rysza\\Zotero\\storage\\LT2HGC5I\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf},
}

@misc{von_platen_diffusers_2025,
	title = {Diffusers: {State}-of-the-art diffusion models},
	copyright = {Apache-2.0},
	shorttitle = {Diffusers},
	url = {https://github.com/huggingface/diffusers},
	abstract = {🤗 Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch.},
	urldate = {2025-12-19},
	author = {von Platen, Patrick and Patil, Suraj and Lozhkov, Anton and Cuenca, Pedro and Lambert, Nathan and Rasul, Kashif and Davaadorj, Mishig and Nair, Dhruv and Paul, Sayak and Liu, Steven and Berman, William and Xu, Yiyi and Wolf, Thomas},
	month = dec,
	year = {2025},
	note = {original-date: 2022-05-30T16:04:02Z},
}

@misc{noauthor_wan-aiwan22-i2v-a14b-diffusers_2025,
	title = {Wan-{AI}/{Wan2}.2-{I2V}-{A14B}-{Diffusers} · {Hugging} {Face}},
	url = {https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-12-19},
	month = aug,
	year = {2025},
	file = {Snapshot:C\:\\Users\\rysza\\Zotero\\storage\\TSPUAPKD\\Wan2.html:text/html},
}
